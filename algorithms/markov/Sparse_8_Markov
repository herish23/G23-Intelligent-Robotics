"""
Markov Grid Based Localization (GPU)

Main script to run Grid-Based (Markov) Localization, accelerated with CUDA/GPU.

** This version is now FULLY SELF-CONTAINED and uses CuPy for GPU acceleration. **
It requires an NVIDIA GPU, the CUDA Toolkit, and the 'cupy' library.

**Changelog:**
- Added 'GPU_ENABLED' flag to create a robust CPU-fallback mode.
- Fixed 'AttributeError' crash when CuPy is not found.
- Added 'scipy' as a standard import.
- Added final average error calculation.
- Added Gaussian belief initialization (using your 'hint').
- REMOVED: All matplotlib plotting.
- ADDED: Saves all results to a CSV file on your desktop.
- CHANGED: Batch interval from 50 to 8.
"""

import numpy as np
import time
import csv
import math
import sys
import random 
import os 
import yaml 
from PIL import Image 
from dataclasses import dataclass 
from scipy.ndimage import distance_transform_edt as cpu_distance_transform_edt
import scipy

# --- Try to import GPU libraries ---
try:
    # We need both cupy and the cupy-enabled scipy extensions
    import cupy as cp
    from cupyx.scipy.ndimage import distance_transform_edt as gpu_distance_transform_edt
    GPU_ENABLED = True
    print("Successfully imported CuPy. GPU acceleration is ENABLED.")
except ImportError:
    GPU_ENABLED = False
    print("="*70)
    print("Warning: Could not import 'cupy' or 'cupyx.scipy'.")
    print("GPU acceleration will be DISABLED. Script will run on CPU.")
    print("Please ensure you have an NVIDIA GPU, the CUDA Toolkit, and 'cupy' installed.")
    print("  (e.g., 'pip install cupy-cuda12x scipy')")
    print("="*70)
    # Fallback to CPU-only
    cp = np # Use numpy as a substitute
    # We create a simple wrapper to make the CPU function look like the GPU one
    def gpu_distance_transform_edt(arr):
        return cpu_distance_transform_edt(arr)


# -----------------------------------------------------------------------------
# CONFIGURATION
# -----------------------------------------------------------------------------
# All settings are in this dictionary. Update the paths to match your system.
# *** IMPORTANT: All 3 paths MUST be FULL, ABSOLUTE paths ***
# -----------------------------------------------------------------------------
CONFIG = {
    'data': {
        # --- 1. UPDATE THIS PATH ---
        # Must be the FULL, ABSOLUTE path to your sensor data
        'sensor_data_csv_path': r'C:\Users\Rohit\Desktop\Intelligent Robotics\G23-Intelligent-Robotics\data\sensor_data_clean.csv',
        
        # --- 2. UPDATE THESE PATHS ---
        # Must be the FULL, ABSOLUTE paths to your map files
        'map_pgm_path': r'C:\Users\Rohit\Desktop\Intelligent Robotics\G23-Intelligent-Robotics\maps\epuck_world_map.pgm',
        'map_yaml_path': r'C:\Users\Rohit\Desktop\Intelligent Robotics\G23-Intelligent-Robotics\maps\epuck_world_map.yaml',
        
        # --- 3. NEW: Define output CSV path ---
        # This will save the results to your Desktop.
        'output_csv_path': os.path.join(os.path.expanduser('~'), 'Desktop', 'localization_results.csv')
    },
    'sensors': {
        'lidar': {
            'num_rays': 360,     # Total rays in the scan
            'fov_deg': 360,      # Field of view in degrees
            'sparsity': 10,      # Use 1-in-10 rays (36 total) for speed.
        }
    },
    'localization': {
        'likelihood_sigma': 0.1,     # Sharpened sensor model (10cm blur)
        'likelihood_max_dist': 2.0,  # Max distance for likelihood (meters)
        'motion_diffusion_factor': 0.01, # Trust our (perfect) motion model more
        
        # --- NEW: POSE HINT ---
        # Set 'use_pose_hint' to True to use this.
        'use_pose_hint': True,
        'initial_pose_hint': [-0.46, -0.55],     # (x_m, y_m)
        'initial_uncertainty_m': 0.5         # Standard deviation (0.5m)
        # --- END OF NEW HINT ---
    }
}
# -----------------------------------------------------------------------------
# END OF CONFIGURATION
# -----------------------------------------------------------------------------


# -----------------------------------------------------------------------------
# HELPER FUNCTIONS (Previously in localization_utils)
# -----------------------------------------------------------------------------

@dataclass
class MapInfo:
    """ Container for map metadata """
    # This stores the map on the GPU if cupy is available
    occupancy_grid: cp.ndarray 
    resolution: float
    origin_x: float
    origin_y: float
    width: int
    height: int
    occupied_thresh: float
    free_thresh: float

def load_map(pgm_path, yaml_path):
    """ load map from pgm and yaml files """
    # load yaml metadata
    with open(yaml_path, 'r') as f:
        meta = yaml.safe_load(f)

    res = meta['resolution']
    origin = meta['origin']
    ox, oy = origin[0], origin[1]
    occ_thresh = meta.get('occupied_thresh', 0.65)
    free_thresh = meta.get('free_thresh', 0.25)
    negate = meta.get('negate', 0)

    # load pgm image using numpy (CPU)
    try:
        img = Image.open(pgm_path)
    except FileNotFoundError:
        print(f"FATAL ERROR: Map image file not found at {pgm_path}")
        return None
    img_arr_cpu = np.array(img)

    # convert to occupancy grid - trinary mode
    # pgm: 255=white=free, 0=black=occupied
    norm = img_arr_cpu / 255.0
    occ_grid_cpu = np.where(norm > (1 - free_thresh), 0.0,
                        np.where(norm < occ_thresh, 1.0, 0.5))

    h, w = occ_grid_cpu.shape
    
    # --- GPU CHANGE ---
    # Move the final occupancy grid to the GPU (or keep as numpy if disabled)
    occ_grid_gpu = cp.asarray(occ_grid_cpu)

    return MapInfo(
        occupancy_grid=occ_grid_gpu, # Store the GPU/CPU array
        resolution=res,
        origin_x=ox,
        origin_y=oy,
        width=w,
        height=h,
        occupied_thresh=occ_thresh,
        free_thresh=free_thresh
    )

def load_sensor_data(config):
    """
    Loads clean sensor data from the specified CSV file.
    This function still runs on the CPU.
    """
    csv_file_path = config['data']['sensor_data_csv_path']
    num_rays = config['sensors']['lidar']['num_rays']
    
    sensor_log = []
    
    print(f"Loading sensor data from: {csv_file_path}")
    try:
        with open(csv_file_path, 'r') as f:
            reader = csv.reader(f)
            header = next(reader)
            
            # Find column indices
            try:
                ts_idx = header.index('timestamp')
                range_start_idx = header.index('range_0')
                gt_x_idx = header.index('gt_x')
                gt_y_idx = header.index('gt_y')
                gt_theta_idx = header.index('gt_theta')
            except ValueError as e:
                print(f"Error: Missing required column in CSV: {e}")
                print(f"Header found: {header}")
                return None
                
            range_end_idx = range_start_idx + num_rays
            if range_end_idx > len(header):
                print(f"Error: CSV header does not have {num_rays} 'range_...' columns.")
                return None

            # Read data
            for row in reader:
                try:
                    data_point = {
                        'timestamp': float(row[ts_idx]),
                        # Keep ranges and pose as numpy arrays for now
                        # We will move them to the GPU one by one in the loop
                        'ranges': np.array([float(r) for r in row[range_start_idx:range_end_idx]]),
                        'pose': (
                            float(row[gt_x_idx]),
                            float(row[gt_y_idx]),
                            float(row[gt_theta_idx])
                        )
                    }
                    sensor_log.append(data_point)
                except (ValueError, IndexError):
                    print(f"Warning: Skipping malformed row: {row}")
                    
    except FileNotFoundError:
        print(f"FATAL ERROR: Sensor data file not found at: {csv_file_path}")
        print("Please check the 'sensor_data_csv_path' in the CONFIG dictionary.")
        return None
    except Exception as e:
        print(f"Error loading sensor data: {e}")
        return None

    return sensor_log

def world_to_grid(x, y, map_info):
    """ world coords (meters) to grid coords (pixels) - CPU """
    grid_x = int(round((x - map_info.origin_x) / map_info.resolution))
    grid_y = int(round((y - map_info.origin_y) / map_info.resolution))
    return grid_x, grid_y

def grid_to_world(grid_x, grid_y, map_info):
    """ grid coords to world coords - CPU """
    # We add 0.5 to get the center of the cell
    x = (grid_x + 0.5) * map_info.resolution + map_info.origin_x
    y = (grid_y + 0.5) * map_info.resolution + map_info.origin_y
    return x, y

def gaussian_2d(x, y, mu_x, mu_y, sigma):
    """
    Calculates 2D Gaussian probability (vectorized for GPU/CPU).
    Assumes sigma_x = sigma_y = sigma.
    """
    sigma_sq = sigma**2
    top = cp.exp(-0.5 * (((x - mu_x)**2 / sigma_sq) + ((y - mu_y)**2 / sigma_sq)))
    bottom = 2 * cp.pi * sigma_sq
    return top / bottom

def compute_likelihood_field(map_info, sigma=0.1, max_dist=2.0):
    """ precompute likelihood field for sensor model - GPU/CPU """
    
    # --- GPU CHANGE ---
    # map_info.occupancy_grid is already on the GPU (or is numpy)
    obs = (map_info.occupancy_grid > 0.5).astype(cp.float32)

    # distance transform - euclidean dist to nearest obstacle
    # Use the imported GPU/CPU version
    dist = gpu_distance_transform_edt(1 - obs)

    # convert to meters
    dist_m = dist * map_info.resolution

    # gaussian around obstacles
    sig_cells = sigma / map_info.resolution
    
    # --- GPU CHANGE --- (use cp.exp)
    field = cp.exp(-0.5 * (dist / sig_cells) ** 2)

    # clamp far ditsances
    max_cells = max_dist / map_info.resolution
    field[dist > max_cells] = 0.01

    # normalize
    field = field / field.max()

    return field

def compute_scan_likelihood_field(scan_ranges_gpu, scan_angles_gpu, robot_x, robot_y, robot_theta,
                                  map_info, likelihood_field_gpu):
    """
    Compute total likelihood of a scan using pre-computed likelihood field.
    Returns the *log-likelihood*.
    This function is a mix of CPU/GPU - can be further optimized.
    """
    log_likelihood = 0.0
    num_valid = 0

    # --- GPU CHANGE ---
    # Move scalar values to GPU for calculations
    robot_x_gpu = cp.float32(robot_x)
    robot_y_gpu = cp.float32(robot_y)
    robot_theta_gpu = cp.float32(robot_theta)

    # --- GPU CHANGE ---
    # Perform calculations on GPU
    global_angles = robot_theta_gpu + scan_angles_gpu
    px_gpu = robot_x_gpu + scan_ranges_gpu * cp.cos(global_angles)
    py_gpu = robot_y_gpu + scan_ranges_gpu * cp.sin(global_angles)

    # --- GPU/CPU CHANGE ---
    # This is a bottleneck: moving data back from GPU to CPU.
    # If GPU is disabled, this is a simple numpy array copy.
    if GPU_ENABLED:
        px_cpu = px_gpu.get()
        py_cpu = py_gpu.get()
        scan_ranges_cpu = scan_ranges_gpu.get()
        likelihood_field_cpu = likelihood_field_gpu.get()
    else:
        px_cpu = px_gpu # It's already a numpy array
        py_cpu = py_gpu
        scan_ranges_cpu = scan_ranges_gpu
        likelihood_field_cpu = likelihood_field_gpu

    for r, px, py in zip(scan_ranges_cpu, px_cpu, py_cpu):
        # Skip invalid ranges
        if r < 0.12 or r > 3.5:
            continue

        # Convert to grid (on CPU)
        gx, gy = world_to_grid(px, py, map_info) # Calls local function

        # Check bounds
        if 0 <= gx < map_info.width and 0 <= gy < map_info.height:
            # Lookup pre-computed likelihood (on CPU)
            p = likelihood_field_cpu[gy, gx]
            # Avoid log(0)
            p = max(p, 1e-10)
            log_likelihood += np.log(p) # Use np.log on CPU
            num_valid += 1

    # Normalize by number of valid beams
    if num_valid > 0:
        return log_likelihood / num_valid
    else:
        return 0.0 # No valid beams

# -----------------------------------------------------------------------------
# END OF HELPER FUNCTIONS
# -----------------------------------------------------------------------------


def prediction_step(belief_grid_gpu, motion, map_info, diffusion_factor):
    """
    Updates the belief grid based on a motion model P(x_t | u_t, x_t-1).
    'motion' is (dx_m, dy_m). Runs on GPU/CPU.
    """
    dx_m, dy_m = motion
    
    # Convert motion from meters to grid cells
    dx_cells = int(round(dx_m / map_info.resolution))
    dy_cells = int(round(dy_m / map_info.resolution))
    
    # --- GPU CHANGE ---
    # This is a simple "shift" model. (use cp.roll)
    predicted_belief = cp.roll(belief_grid_gpu, (dy_cells, dx_cells), axis=(0, 1))
    
    # Zero out probabilities that "leaked" from the other side
    if dy_cells > 0:
        predicted_belief[:dy_cells, :] = 0
    elif dy_cells < 0:
        predicted_belief[dy_cells:, :] = 0
    if dx_cells > 0:
        predicted_belief[:, :dx_cells] = 0
    elif dx_cells < 0:
        predicted_belief[:, dx_cells:] = 0

    # Add a simple diffusion model for motion noise
    uniform_prob = 1.0 / belief_grid_gpu.size
    final_belief = predicted_belief * (1.0 - diffusion_factor) + \
                   (uniform_prob * diffusion_factor)
    
    # Mask out any probabilities in occupied cells
    # map_info.occupancy_grid is already on GPU/CPU
    final_belief[map_info.occupancy_grid > 0.5] = 0.0

    # Normalize the belief
    total_prob = cp.sum(final_belief)
    if total_prob > 0:
        return final_belief / total_prob
    else:
        # Failsafe: re-initialize if all probability is lost
        print("  Warning: Prediction step resulted in zero probability. Re-initializing.")
        final_belief = cp.ones_like(belief_grid_gpu)
        final_belief[map_info.occupancy_grid > 0.5] = 0.0
        return final_belief / cp.sum(final_belief)

def correction_step(belief_grid_gpu, sensor_data, map_info, likelihood_field_gpu, config, verbose=False):
    """
    Updates the belief grid based on the sensor model P(z_t | x_t)
    using the pre-computed likelihood field. Runs on GPU/CPU.
    """
    
    lidar_config = config['sensors']['lidar']
    
    # --- GPU CHANGE ---
    # Move sensor data to GPU (or keep as numpy if disabled)
    lidar_ranges_gpu = cp.asarray(sensor_data['ranges'])
    
    # Create an array of lidar angles (on GPU/CPU)
    angles_gpu = cp.linspace(
        -math.radians(lidar_config['fov_deg']) / 2.0,
        math.radians(lidar_config['fov_deg']) / 2.0,
        lidar_config['num_rays']
    )
    
    # Apply sparsity
    sparse_indices = cp.arange(0, len(lidar_ranges_gpu), lidar_config['sparsity'])
    sparse_ranges_gpu = lidar_ranges_gpu[sparse_indices]
    sparse_angles_gpu = angles_gpu[sparse_indices]

    # ** This is the 2D "cheat" **
    gt_theta_rad = sensor_data['pose'][2]

    # Create a likelihood grid (log-probabilities)
    log_likelihood_grid = cp.zeros_like(belief_grid_gpu)
    
    if verbose:
        print("  Running correction step (using likelihood field)...")
    
    # Create grid of all possible cell coordinates (on GPU/CPU)
    y_coords, x_coords = cp.indices(belief_grid_gpu.shape)
    
    # --- GPU/CPU BUG FIX ---
    # Convert all grid coords to world coords (on CPU)
    # This is a bottleneck!
    if GPU_ENABLED:
        x_coords_cpu = x_coords.get()
        y_coords_cpu = y_coords.get()
    else:
        x_coords_cpu = x_coords # It's already a numpy array
        y_coords_cpu = y_coords
        
    # We must use the numpy-based grid_to_world for this array operation
    wx_cpu = (x_coords_cpu + 0.5) * map_info.resolution + map_info.origin_x
    wy_cpu = (y_coords_cpu + 0.5) * map_info.resolution + map_info.origin_y
    
    # Move world coords back to GPU
    wx = cp.asarray(wx_cpu)
    wy = cp.asarray(wy_cpu)
    
    # Initialize log-likelihoods
    log_likelihood_grid = cp.full_like(belief_grid_gpu, -cp.inf) # Start all at -inf
    
    # Find all valid cells (not in an obstacle AND have some belief)
    valid_mask = (map_info.occupancy_grid < 0.5) & (belief_grid_gpu > 1e-9)
    
    valid_y = y_coords[valid_mask]
    valid_x = x_coords[valid_mask]
    valid_wx = wx[valid_mask]
    valid_wy = wy[valid_mask]

    # Calculate log-likelihood for all valid cells
    # This loop is the main bottleneck.
    log_p_values = []
    
    # --- GPU/CPU BUG FIX ---
    if GPU_ENABLED:
        valid_wx_cpu = valid_wx.get()
        valid_wy_cpu = valid_wy.get()
    else:
        valid_wx_cpu = valid_wx
        valid_wy_cpu = valid_wy
        
    for wx_cell, wy_cell in zip(valid_wx_cpu, valid_wy_cpu): # .get() to iterate on CPU
        log_p = compute_scan_likelihood_field(
            sparse_ranges_gpu, sparse_angles_gpu, 
            wx_cell, wy_cell, gt_theta_rad,
            map_info, likelihood_field_gpu
        )
        log_p_values.append(log_p)
    
    # Assign computed log-likelihoods back to the grid
    if len(valid_y) > 0:
        log_likelihood_grid[valid_y, valid_x] = cp.asarray(log_p_values)
    
    if verbose:
        print(f"  Correction step done. (Computed likelihood for {len(valid_y)} cells)")

    # Convert from log-probability to probability
    if len(valid_y) > 0:
        max_log_like = cp.max(log_likelihood_grid[cp.isfinite(log_likelihood_grid)])
    else:
        max_log_like = 0.0 # No valid cells, nothing to compute
    
    if not cp.isfinite(max_log_like):
        if verbose:
            print("  Warning: All cells have -inf log-likelihood. Belief may collapse.")
        max_log_like = 0.0

    likelihood_grid = cp.exp(log_likelihood_grid - max_log_like)
    
    # Update belief: Bel(x_t) = eta * P(z_t | x_t) * Bel_bar(x_t)
    updated_belief = belief_grid_gpu * likelihood_grid
    
    # Normalize the final belief
    total_prob = cp.sum(updated_belief)
    if total_prob > 0:
        return updated_belief / total_prob
    else:
        # Failsafe
        if verbose:
            print("  Warning: Belief collapsed. Re-initializing.")
        final_belief = cp.ones_like(belief_grid_gpu)
        final_belief[map_info.occupancy_grid > 0.5] = 0.0
        return final_belief / cp.sum(final_belief)

def main():
    # Use the global CONFIG object
    config = CONFIG 
    print(f"Loading configuration from internal CONFIG dictionary...")
    
    # --- 1. Load Map ---
    print("\nLoading map...")
    try:
        map_info = load_map(
            config['data']['map_pgm_path'], 
            config['data']['map_yaml_path']
        )
        if map_info is None: # Check if load_map failed
            return
        print(f"Map loaded: {map_info.width}x{map_info.height} cells @ {map_info.resolution:.3f} m/cell")
    except FileNotFoundError as e:
        print(f"Error loading map YAML: {e}")
        print("Please ensure 'map_yaml_path' in the global CONFIG dictionary is correct.")
        return
    except Exception as e:
        print(f"An error occurred loading the map: {e}")
        return

    # --- 2. Pre-compute Likelihood Field ---
    print("Pre-computing likelihood field (this may take a moment)...")
    start_time = time.time()
    likelihood_field_gpu = compute_likelihood_field(
        map_info, 
        config['localization']['likelihood_sigma'],
        config['localization']['likelihood_max_dist']
    )
    print(f"Likelihood field computed in {time.time() - start_time:.2f}s")
    
    # --- 3. Initialize Belief (NOW WITH HINT) ---
    print("Initializing belief grid...")
    
    use_hint = config['localization'].get('use_pose_hint', False)
    
    if use_hint:
        hint_x, hint_y = config['localization']['initial_pose_hint']
        hint_std = config['localization']['initial_uncertainty_m']
        print(f"Using initial pose hint: ({hint_x}m, {hint_y}m) with {hint_std}m uncertainty.")

        # Create grid of all cell coordinates (on GPU/CPU)
        y_coords, x_coords = cp.indices((map_info.height, map_info.width))
        
        # Convert all grid coords to world coords (vectorized, on CPU)
        if GPU_ENABLED:
            x_coords_cpu = x_coords.get()
            y_coords_cpu = y_coords.get()
        else:
            x_coords_cpu = x_coords
            y_coords_cpu = y_coords
            
        wx_cpu = (x_coords_cpu + 0.5) * map_info.resolution + map_info.origin_x
        wy_cpu = (y_coords_cpu + 0.5) * map_info.resolution + map_info.origin_y
        
        # Move back to GPU/CPU
        wx = cp.asarray(wx_cpu)
        wy = cp.asarray(wy_cpu)

        # Calculate 2D Gaussian
        belief_grid_gpu = gaussian_2d(wx, wy, hint_x, hint_y, hint_std)

    else:
        print("Using uniform distribution (no pose hint).")
        # P(x_0) = Uniform distribution (on GPU/CPU)
        belief_grid_gpu = cp.ones((map_info.height, map_info.width))
    
    # This part is the same for both cases
    belief_grid_gpu[map_info.occupancy_grid > 0.5] = 0.0
    
    if cp.sum(belief_grid_gpu) == 0:
        print("Error: Map seems to be entirely obstacles (or hint is in a wall). Cannot initialize belief.")
        return
        
    belief_grid_gpu = belief_grid_gpu / cp.sum(belief_grid_gpu)
    # --- END OF NEW INITIALIZE ---
    
    # --- 4. Load Sensor Data ---
    print("Loading sensor data...")
    sensor_log = load_sensor_data(config)
    if not sensor_log:
        print("Failed to load sensor data. Exiting.")
        return
    print(f"Loaded {len(sensor_log)} full data points.")
    
    # --- EDITED: BATCH THE DATA ---
    batch_interval = 8 # <-- EDITED: Changed from 50 to 8
    sensor_log = sensor_log[::batch_interval]
    print(f"Batched data to {len(sensor_log)} steps (1 every {batch_interval}).")
    # --- END OF EDIT ---

    # --- 5. Run Localization Loop ---
    last_gt_pose = None
    total_steps = len(sensor_log)
    
    # --- NEW: Initialize list to store errors ---
    all_position_errors = []
    
    # --- NEW: Setup CSV file for writing ---
    output_csv_path = config['data']['output_csv_path']
    print(f"Opening output CSV file at: {output_csv_path}")
    try:
        csv_file = open(output_csv_path, 'w', newline='')
        csv_writer = csv.writer(csv_file)
        # Write header
        csv_writer.writerow([
            "timestamp", "gt_x", "gt_y", 
            "est_x", "est_y", "position_error_m", "max_belief"
        ])
    except IOError as e:
        print(f"FATAL ERROR: Could not open CSV file for writing: {e}")
        print("Please check the path and permissions.")
        return
    # --- END OF CSV SETUP ---

    for i, sensor_data in enumerate(sensor_log):
        print(f"\n--- Processing Timestamp {sensor_data['timestamp']} (Step {i+1}/{total_steps}) ---")
        
        start_step_time = time.time()
        
        current_gt_pose = sensor_data['pose']
        gt_x_m, gt_y_m, gt_theta_rad = current_gt_pose
        
        gt_grid_x, gt_grid_y = world_to_grid(gt_x_m, gt_y_m, map_info)

        # --- Prediction Step (Motion Model) ---
        if last_gt_pose:
            dx_m = current_gt_pose[0] - last_gt_pose[0]
            dy_m = current_gt_pose[1] - last_gt_pose[1]
            motion = (dx_m, dy_m)
            
            print(f"  Applying motion: dx={dx_m:.2f}m, dy={dy_m:.2f}m")
                
            belief_grid_gpu = prediction_step(
                belief_grid_gpu, 
                motion, 
                map_info, 
                config['localization']['motion_diffusion_factor']
            )
        else:
            print("  Skipping prediction step (first frame).")

        # --- Correction Step (Sensor Model) ---
        belief_grid_gpu = correction_step(
            belief_grid_gpu, 
            sensor_data, 
            map_info, 
            likelihood_field_gpu, 
            config,
            verbose=True # Always print correction step details
        )

        # --- Find Best Estimate & Print ---
        # --- GPU/CPU BUG FIX ---
        # Move data back to CPU for argmax and printing (if it was on GPU)
        if GPU_ENABLED:
            belief_grid_cpu = belief_grid_gpu.get()
        else:
            belief_grid_cpu = belief_grid_gpu # It's already a numpy array
        
        est_grid_y, est_grid_x = np.unravel_index(
            np.argmax(belief_grid_cpu), 
            belief_grid_cpu.shape
        )
        est_x_m, est_y_m = grid_to_world(est_grid_x, est_grid_y, map_info)
        max_prob = np.max(belief_grid_cpu)
        
        prob_at_gt = 0.0
        if 0 <= gt_grid_x < map_info.width and 0 <= gt_grid_y < map_info.height:
             # --- TYPO FIX: Changed prob_at_gty to prob_at_gt ---
             prob_at_gt = belief_grid_cpu[gt_grid_y, gt_grid_x]
        else:
            print(f"  Warning: Ground truth ({gt_grid_x}, {gt_grid_y}) is outside map bounds (0-{map_info.width}, 0-{map_info.height}).")

        # --- 6. Print and Compare ---
        print("\n  --- Results ---")
        print(f"  Step Time: {time.time() - start_step_time:.2f}s")
        print(f"  Ground Truth (World): ({gt_x_m:.2f}m, {gt_y_m:.2f}m)")
        print(f"  Ground Truth (Grid):  ({gt_grid_x}, {gt_grid_y})")
        print(f"  Estimated Pos (World): ({est_x_m:.2f}m, {est_y_m:.2f}m)")
        print(f"  Estimated Pos (Grid):  ({est_grid_x}, {est_grid_y})")
        
        # --- NEW: Calculate and store error ---
        current_error_m = math.dist((gt_x_m, gt_y_m), (est_x_m, est_y_m))
        all_position_errors.append(current_error_m)
        print(f"  Position Error: {current_error_m:.3f} m")
        # --- END OF NEW BLOCK ---
        
        print(f"  Highest Belief: {max_prob:.4e}")
        print(f"  Belief at GT:   {prob_at_gt:.4e}")

        # --- 7. NEW: Write to CSV ---
        csv_writer.writerow([
            sensor_data['timestamp'],
            gt_x_m, gt_y_m,
            est_x_m, est_y_m,
            current_error_m,
            max_prob
        ])
        # --- END OF CSV WRITE ---

        # Update last pose for next iteration
        # --- TYPO FIX: Corrected variable name ---
        last_gt_pose = current_gt_pose
    
    print("\n--- Localization Complete ---")

    # --- NEW: Close CSV file ---
    csv_file.close()
    print(f"Results saved to {output_csv_path}")
    # --- END OF CSV CLOSE ---

    # --- NEW: Calculate and print average error ---
    if all_position_errors: # Avoid division by zero
        average_error_m = sum(all_position_errors) / len(all_position_errors)
        print("\n--- Final Summary ---")
        print(f"Processed {len(all_position_errors)} batched steps (1 every {batch_interval}).")
        print(f"Average Position Error: {average_error_m:.3f} m")
    # --- END OF NEW BLOCK ---

if __name__ == "__main__":
    main()