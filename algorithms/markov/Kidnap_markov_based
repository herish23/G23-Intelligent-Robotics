"""
Markov Grid Based Localization (GPU) - Kidnapped Robot Experiment

This script runs a localization experiment where the robot is "kidnapped"
(its belief is reset to uniform) at a specific timestep to test recovery.

SETTINGS:
- Noise: 10% (Integrated)
- Sparsity: 10
- Batch Interval: 1 (To capture detailed recovery)
- Kidnapping: ENABLED at step 500
"""

import numpy as np
import time
import csv
import math
import sys
import random 
import os 
import yaml 
from PIL import Image 
from dataclasses import dataclass 
from scipy.ndimage import distance_transform_edt as cpu_distance_transform_edt
import scipy

# --- Try to import GPU libraries ---
try:
    import cupy as cp
    from cupyx.scipy.ndimage import distance_transform_edt as gpu_distance_transform_edt
    GPU_ENABLED = True
    print("Successfully imported CuPy. GPU acceleration is ENABLED.")
except ImportError:
    GPU_ENABLED = False
    print("="*70)
    print("Warning: Could not import 'cupy' or 'cupyx.scipy'.")
    print("GPU acceleration will be DISABLED. Script will run on CPU.")
    print("="*70)
    cp = np 
    def gpu_distance_transform_edt(arr):
        return cpu_distance_transform_edt(arr)


# -----------------------------------------------------------------------------
# CONFIGURATION
# -----------------------------------------------------------------------------
CONFIG = {
    'experiment': {
        'name': "kidnap_recovery_noise10_sparse10"
    },
    'data': {
        'sensor_data_csv_path': r'C:\Users\Rohit\Desktop\Intelligent Robotics\G23-Intelligent-Robotics\data\sensor_data_clean.csv',
        'map_pgm_path': r'C:\Users\Rohit\Desktop\Intelligent Robotics\G23-Intelligent-Robotics\maps\epuck_world_map.pgm',
        'map_yaml_path': r'C:\Users\Rohit\Desktop\Intelligent Robotics\G23-Intelligent-Robotics\maps\epuck_world_map.yaml',
        'output_csv_path': os.path.join(os.path.expanduser('~'), 'Desktop', 'localization_results_kidnap_noise10.csv')
    },
    'sensors': {
        'lidar': {
            'num_rays': 360,
            'fov_deg': 360,
            'max_range': 3.5,
            
            # --- 10% NOISE ENABLED ---
            'noise_std': 0.02,     
            'outlier_rate': 0.10, 
            # -------------------------
            
            'sparsity': 10, 
        }
    },
    'localization': {
        'likelihood_sigma': 0.1,
        'likelihood_max_dist': 2.0,
        'motion_diffusion_factor': 0.01,
        
        'use_pose_hint': True,
        'initial_pose_hint': [-0.46, -0.55],
        'initial_uncertainty_m': 0.5,
        
        # Use 1 to see the recovery curve clearly
        'batch_interval': 1 
    },
    # --- KIDNAPPING ENABLED ---
    'kidnapping': {
        'enabled': True,           # <-- ENABLED
        'timestep': 500,           # Trigger kidnap at step 500
        'markov': {
            'reset_mode': "uniform" # Reset to uniform distribution
        }
    },
    'random_seed': 42
}
# -----------------------------------------------------------------------------


# -----------------------------------------------------------------------------
# HELPER FUNCTIONS
# -----------------------------------------------------------------------------

@dataclass
class MapInfo:
    """ Container for map metadata """
    occupancy_grid: cp.ndarray 
    resolution: float
    origin_x: float
    origin_y: float
    width: int
    height: int
    occupied_thresh: float
    free_thresh: float

def load_map(pgm_path, yaml_path):
    """ load map from pgm and yaml files """
    with open(yaml_path, 'r') as f:
        meta = yaml.safe_load(f)

    res = meta['resolution']
    origin = meta['origin']
    ox, oy = origin[0], origin[1]
    occ_thresh = meta.get('occupied_thresh', 0.65)
    free_thresh = meta.get('free_thresh', 0.25)
    negate = meta.get('negate', 0)

    try:
        img = Image.open(pgm_path)
    except FileNotFoundError:
        print(f"FATAL ERROR: Map image file not found at {pgm_path}")
        return None
    img_arr_cpu = np.array(img)

    norm = img_arr_cpu / 255.0
    occ_grid_cpu = np.where(norm > (1 - free_thresh), 0.0,
                        np.where(norm < occ_thresh, 1.0, 0.5))

    h, w = occ_grid_cpu.shape
    occ_grid_gpu = cp.asarray(occ_grid_cpu)

    return MapInfo(
        occupancy_grid=occ_grid_gpu, 
        resolution=res,
        origin_x=ox,
        origin_y=oy,
        width=w,
        height=h,
        occupied_thresh=occ_thresh,
        free_thresh=free_thresh
    )

def apply_sensor_noise_internal(clean_log, config):
    """
    Applies noise to a clean sensor log based on config parameters.
    """
    print("Applying sensor noise based on config settings...")
    lidar_config = config['sensors']['lidar']
    noise_std = lidar_config.get('noise_std', 0.0)
    outlier_rate = lidar_config.get('outlier_rate', 0.0)
    max_range = lidar_config.get('max_range', 3.5)
    
    seed = config.get('random_seed', 42)
    rng = np.random.RandomState(seed)
    
    noisy_log = []
    
    for data in clean_log:
        clean_ranges = data['ranges']
        noisy_ranges = clean_ranges.copy()
        num_rays = len(noisy_ranges)

        # 1. Apply Gaussian noise
        if noise_std > 0:
            noise = rng.normal(0.0, noise_std, num_rays)
            noisy_ranges += noise
        
        # 2. Apply Outlier noise
        if outlier_rate > 0:
            num_outliers = int(num_rays * outlier_rate)
            if num_outliers > 0:
                outlier_indices = rng.choice(num_rays, num_outliers, replace=False)
                random_values = rng.uniform(0.0, max_range, num_outliers)
                noisy_ranges[outlier_indices] = random_values

        # Clip to valid sensor range
        noisy_ranges = np.clip(noisy_ranges, 0.0, max_range)

        noisy_log.append({
            'timestamp': data['timestamp'],
            'ranges': noisy_ranges,
            'pose': data['pose']
        })
        
    print(f"Applied Gaussian noise (std={noise_std}) and outlier noise (rate={outlier_rate})")
    return noisy_log

def load_sensor_data(config):
    csv_file_path = config['data']['sensor_data_csv_path']
    num_rays = config['sensors']['lidar'].get('num_rays', 360) 
    clean_sensor_log = []
    print(f"Loading CLEAN sensor data from: {csv_file_path}")
    try:
        with open(csv_file_path, 'r') as f:
            reader = csv.reader(f)
            header = next(reader)
            try:
                ts_idx = header.index('timestamp')
                range_start_idx = header.index('range_0')
                gt_x_idx = header.index('gt_x')
                gt_y_idx = header.index('gt_y')
                gt_theta_idx = header.index('gt_theta')
            except ValueError as e:
                print(f"Error: Missing required column in CSV: {e}")
                return None
            range_end_idx = range_start_idx + num_rays
            
            for row in reader:
                try:
                    data_point = {
                        'timestamp': float(row[ts_idx]),
                        'ranges': np.array([float(r) for r in row[range_start_idx:range_end_idx]]),
                        'pose': (
                            float(row[gt_x_idx]),
                            float(row[gt_y_idx]),
                            float(row[gt_theta_idx])
                        )
                    }
                    clean_sensor_log.append(data_point)
                except (ValueError, IndexError):
                    pass 
    except FileNotFoundError:
        print(f"FATAL ERROR: Sensor data file not found at: {csv_file_path}")
        return None

    # Apply noise using internal function
    noisy_sensor_log = apply_sensor_noise_internal(clean_sensor_log, config)
    return noisy_sensor_log

def world_to_grid(x, y, map_info):
    grid_x = int(round((x - map_info.origin_x) / map_info.resolution))
    grid_y = int(round((y - map_info.origin_y) / map_info.resolution))
    return grid_x, grid_y

def grid_to_world(grid_x, grid_y, map_info):
    x = (grid_x + 0.5) * map_info.resolution + map_info.origin_x
    y = (grid_y + 0.5) * map_info.resolution + map_info.origin_y
    return x, y

def gaussian_2d(x, y, mu_x, mu_y, sigma):
    sigma_sq = sigma**2
    top = cp.exp(-0.5 * (((x - mu_x)**2 / sigma_sq) + ((y - mu_y)**2 / sigma_sq)))
    bottom = 2 * cp.pi * sigma_sq
    return top / bottom

def compute_likelihood_field(map_info, sigma=0.1, max_dist=2.0):
    obs = (map_info.occupancy_grid > 0.5).astype(cp.float32)
    dist = gpu_distance_transform_edt(1 - obs)
    dist_m = dist * map_info.resolution
    sig_cells = sigma / map_info.resolution
    
    # Beam Model Mixture (Robustness)
    # This is crucial for handling the 10% noise!
    p_hit = cp.exp(-0.5 * (dist / sig_cells) ** 2)
    z_hit = 0.95
    z_rand = 0.05
    p_total = (z_hit * p_hit) + z_rand
    
    field = p_total / p_total.max()
    return field

def compute_scan_likelihood_field(scan_ranges_gpu, scan_angles_gpu, robot_x, robot_y, robot_theta,
                                  map_info, likelihood_field_gpu, max_range):
    log_likelihood = 0.0
    num_valid = 0

    robot_x_gpu = cp.float32(robot_x)
    robot_y_gpu = cp.float32(robot_y)
    robot_theta_gpu = cp.float32(robot_theta)

    global_angles = robot_theta_gpu + scan_angles_gpu
    px_gpu = robot_x_gpu + scan_ranges_gpu * cp.cos(global_angles)
    py_gpu = robot_y_gpu + scan_ranges_gpu * cp.sin(global_angles)

    if GPU_ENABLED:
        px_cpu = px_gpu.get()
        py_cpu = py_gpu.get()
        scan_ranges_cpu = scan_ranges_gpu.get()
        likelihood_field_cpu = likelihood_field_gpu.get()
    else:
        px_cpu = px_gpu 
        py_cpu = py_gpu
        scan_ranges_cpu = scan_ranges_gpu
        likelihood_field_cpu = likelihood_field_gpu

    for r, px, py in zip(scan_ranges_cpu, px_cpu, py_cpu):
        if r < 0.12 or r >= max_range * 0.99: 
            continue
        gx, gy = world_to_grid(px, py, map_info) 
        if 0 <= gx < map_info.width and 0 <= gy < map_info.height:
            p = likelihood_field_cpu[gy, gx]
            p = max(p, 1e-10)
            log_likelihood += np.log(p) 
            num_valid += 1

    if num_valid > 0: return log_likelihood / num_valid
    else: return 0.0 

def prediction_step(belief_grid_gpu, motion, map_info, diffusion_factor):
    dx_m, dy_m = motion
    dx_cells = int(round(dx_m / map_info.resolution))
    dy_cells = int(round(dy_m / map_info.resolution))
    
    predicted_belief = cp.roll(belief_grid_gpu, (dy_cells, dx_cells), axis=(0, 1))
    
    if dy_cells > 0: predicted_belief[:dy_cells, :] = 0
    elif dy_cells < 0: predicted_belief[dy_cells:, :] = 0
    if dx_cells > 0: predicted_belief[:, :dx_cells] = 0
    elif dx_cells < 0: predicted_belief[:, dx_cells:] = 0

    uniform_prob = 1.0 / belief_grid_gpu.size
    final_belief = predicted_belief * (1.0 - diffusion_factor) + (uniform_prob * diffusion_factor)
    final_belief[map_info.occupancy_grid > 0.5] = 0.0

    total_prob = cp.sum(final_belief)
    if total_prob > 0:
        return final_belief / total_prob
    else:
        final_belief = cp.ones_like(belief_grid_gpu)
        final_belief[map_info.occupancy_grid > 0.5] = 0.0
        return final_belief / cp.sum(final_belief)

def correction_step(belief_grid_gpu, sensor_data, map_info, likelihood_field_gpu, config, verbose=False):
    lidar_config = config['sensors']['lidar']
    lidar_ranges_gpu = cp.asarray(sensor_data['ranges'])
    angles_gpu = cp.linspace(-math.radians(lidar_config['fov_deg']) / 2.0,
                             math.radians(lidar_config['fov_deg']) / 2.0,
                             lidar_config['num_rays'])
    
    sparsity = lidar_config.get('sparsity', 1)
    sparse_indices = cp.arange(0, len(lidar_ranges_gpu), sparsity)
    sparse_ranges_gpu = lidar_ranges_gpu[sparse_indices]
    sparse_angles_gpu = angles_gpu[sparse_indices]

    gt_theta_rad = sensor_data['pose'][2]
    log_likelihood_grid = cp.zeros_like(belief_grid_gpu)
    
    y_coords, x_coords = cp.indices(belief_grid_gpu.shape)
    
    if GPU_ENABLED:
        x_coords_cpu = x_coords.get()
        y_coords_cpu = y_coords.get()
    else:
        x_coords_cpu = x_coords 
        y_coords_cpu = y_coords
        
    wx_cpu = (x_coords_cpu + 0.5) * map_info.resolution + map_info.origin_x
    wy_cpu = (y_coords_cpu + 0.5) * map_info.resolution + map_info.origin_y
    
    wx = cp.asarray(wx_cpu)
    wy = cp.asarray(wy_cpu)
    
    log_likelihood_grid = cp.full_like(belief_grid_gpu, -cp.inf) 
    
    # Optimization: only check relevant cells
    valid_mask = (map_info.occupancy_grid < 0.5) & (belief_grid_gpu > 1e-9)
    
    valid_y = y_coords[valid_mask]
    valid_x = x_coords[valid_mask]
    valid_wx = wx[valid_mask]
    valid_wy = wy[valid_mask]

    log_p_values = []
    
    if GPU_ENABLED:
        valid_wx_cpu = valid_wx.get()
        valid_wy_cpu = valid_wy.get()
    else:
        valid_wx_cpu = valid_wx
        valid_wy_cpu = valid_wy
    
    max_range = lidar_config.get('max_range', 3.5)
        
    for wx_cell, wy_cell in zip(valid_wx_cpu, valid_wy_cpu):
        log_p = compute_scan_likelihood_field(
            sparse_ranges_gpu, sparse_angles_gpu, 
            wx_cell, wy_cell, gt_theta_rad,
            map_info, likelihood_field_gpu, max_range 
        )
        log_p_values.append(log_p)
    
    if len(valid_y) > 0:
        log_likelihood_grid[valid_y, valid_x] = cp.asarray(log_p_values)

    if len(valid_y) > 0:
        max_log_like = cp.max(log_likelihood_grid[cp.isfinite(log_likelihood_grid)])
    else:
        max_log_like = 0.0 
    
    if not cp.isfinite(max_log_like):
        max_log_like = 0.0

    likelihood_grid = cp.exp(log_likelihood_grid - max_log_like)
    
    updated_belief = belief_grid_gpu * likelihood_grid
    
    total_prob = cp.sum(updated_belief)
    if total_prob > 0:
        return updated_belief / total_prob
    else:
        final_belief = cp.ones_like(belief_grid_gpu)
        final_belief[map_info.occupancy_grid > 0.5] = 0.0
        return final_belief / cp.sum(final_belief)

def main():
    config = CONFIG 
    print(f"Loading configuration for: {config['experiment']['name']}")
    
    print("\nLoading map...")
    try:
        map_info = load_map(config['data']['map_pgm_path'], config['data']['map_yaml_path'])
        if map_info is None: return
        print(f"Map loaded: {map_info.width}x{map_info.height} cells")
    except Exception as e:
        print(f"An error occurred loading the map: {e}")
        return

    print("Pre-computing likelihood field...")
    start_time = time.time()
    likelihood_field_gpu = compute_likelihood_field(
        map_info, 
        config['localization']['likelihood_sigma'],
        config['localization']['likelihood_max_dist']
    )
    print(f"Likelihood field computed in {time.time() - start_time:.2f}s")
    
    print("Initializing belief grid...")
    use_hint = config['localization'].get('use_pose_hint', False)
    
    if use_hint:
        hint_x, hint_y = config['localization']['initial_pose_hint']
        hint_std = config['localization']['initial_uncertainty_m']
        y_coords, x_coords = cp.indices((map_info.height, map_info.width))
        if GPU_ENABLED:
            x_coords_cpu = x_coords.get()
            y_coords_cpu = y_coords.get()
        else:
            x_coords_cpu = x_coords
            y_coords_cpu = y_coords
        wx_cpu = (x_coords_cpu + 0.5) * map_info.resolution + map_info.origin_x
        wy_cpu = (y_coords_cpu + 0.5) * map_info.resolution + map_info.origin_y
        wx = cp.asarray(wx_cpu)
        wy = cp.asarray(wy_cpu)
        belief_grid_gpu = gaussian_2d(wx, wy, hint_x, hint_y, hint_std)
    else:
        belief_grid_gpu = cp.ones((map_info.height, map_info.width))
    
    belief_grid_gpu[map_info.occupancy_grid > 0.5] = 0.0
    belief_grid_gpu = belief_grid_gpu / cp.sum(belief_grid_gpu)
    
    print("Loading sensor data...")
    sensor_log = load_sensor_data(config)
    if not sensor_log: return
    
    batch_interval = config['localization'].get('batch_interval', 1)
    if batch_interval > 1:
        sensor_log = sensor_log[::batch_interval]
        print(f"Batched data to {len(sensor_log)} steps.")

    last_gt_pose = None
    total_steps = len(sensor_log)
    all_position_errors = []
    
    output_csv_path = config['data']['output_csv_path']
    try:
        csv_file = open(output_csv_path, 'w', newline='')
        csv_writer = csv.writer(csv_file)
        csv_writer.writerow(["timestamp", "gt_x", "gt_y", "est_x", "est_y", "position_error_m", "max_belief", "status"])
    except IOError as e:
        print(f"FATAL ERROR: Could not open CSV file: {e}")
        return

    # --- KIDNAPPING SETUP ---
    kidnap_conf = config.get('kidnapping', {})
    kidnap_enabled = kidnap_conf.get('enabled', False)
    kidnap_timestep = kidnap_conf.get('timestep', -1)
    is_kidnapped = False
    
    for i, sensor_data in enumerate(sensor_log):
        progress = (i + 1) / total_steps
        print(f"\rStep {i+1}/{total_steps} [{'#'*int(progress*20):<20}]", end='')
        
        # --- KIDNAPPING EVENT ---
        if kidnap_enabled and i == kidnap_timestep:
            print(f"\n\n*** KIDNAPPING ROBOT AT STEP {i} ***")
            print("Resetting belief to uniform distribution (Global Uncertainty).")
            belief_grid_gpu = cp.ones((map_info.height, map_info.width))
            belief_grid_gpu[map_info.occupancy_grid > 0.5] = 0.0
            belief_grid_gpu = belief_grid_gpu / cp.sum(belief_grid_gpu)
            is_kidnapped = True
            last_gt_pose = None

        current_gt_pose = sensor_data['pose']
        gt_x_m, gt_y_m, gt_theta_rad = current_gt_pose
        gt_grid_x, gt_grid_y = world_to_grid(gt_x_m, gt_y_m, map_info)

        # Prediction
        if last_gt_pose:
            dx_m = current_gt_pose[0] - last_gt_pose[0]
            dy_m = current_gt_pose[1] - last_gt_pose[1]
            motion = (dx_m, dy_m)
            
            belief_grid_gpu = prediction_step(
                belief_grid_gpu, 
                motion, 
                map_info, 
                config['localization']['motion_diffusion_factor']
            )

        # Correction
        belief_grid_gpu = correction_step(
            belief_grid_gpu, 
            sensor_data, 
            map_info, 
            likelihood_field_gpu, 
            config,
            verbose=False
        )

        # Find Best Estimate
        if GPU_ENABLED: belief_grid_cpu = belief_grid_gpu.get()
        else: belief_grid_cpu = belief_grid_gpu 
        
        est_grid_y, est_grid_x = np.unravel_index(np.argmax(belief_grid_cpu), belief_grid_cpu.shape)
        est_x_m, est_y_m = grid_to_world(est_grid_x, est_grid_y, map_info)
        max_prob = np.max(belief_grid_cpu)
        
        current_error_m = math.dist((gt_x_m, gt_y_m), (est_x_m, est_y_m))
        all_position_errors.append(current_error_m)
        
        status = "KIDNAPPED" if is_kidnapped and i >= kidnap_timestep else "NORMAL"

        csv_writer.writerow([
            sensor_data['timestamp'], gt_x_m, gt_y_m, est_x_m, est_y_m, current_error_m, max_prob, status
        ])

        last_gt_pose = current_gt_pose
    
    csv_file.close()
    print(f"\nResults saved to {output_csv_path}")
    
    if all_position_errors: 
        avg_error = sum(all_position_errors) / len(all_position_errors)
        print(f"Average Position Error: {avg_error:.3f} m")
    print("\n--- Localization Complete ---")

if __name__ == "__main__":
    main()