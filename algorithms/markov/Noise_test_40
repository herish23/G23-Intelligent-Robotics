"""
Markov Grid Based Localization (GPU) - Single File Experiment

This is a self-contained script to run a single localization experiment.
All configuration is in the 'CONFIG' dictionary at the top.
It loads clean data, applies the noise specified in 'CONFIG', 
runs the localization, and saves the results to a CSV.
"""

import numpy as np
import time
import csv
import math
import sys
import random 
import os 
import yaml 
from PIL import Image 
from dataclasses import dataclass 
from scipy.ndimage import distance_transform_edt as cpu_distance_transform_edt
import scipy

# --- Try to import GPU libraries ---
try:
    # We need both cupy and the cupy-enabled scipy extensions
    import cupy as cp
    from cupyx.scipy.ndimage import distance_transform_edt as gpu_distance_transform_edt
    GPU_ENABLED = True
    print("Successfully imported CuPy. GPU acceleration is ENABLED.")
except ImportError:
    GPU_ENABLED = False
    print("="*70)
    print("Warning: Could not import 'cupy' or 'cupyx.scipy'.")
    print("GPU acceleration will be DISABLED. Script will run on CPU.")
    print("Please ensure you have an NVIDIA GPU, the CUDA Toolkit, and 'cupy' installed.")
    print("  (e.g., 'pip install cupy-cuda12x scipy')")
    print("="*70)
    # Fallback to CPU-only
    cp = np # Use numpy as a substitute
    # We create a simple wrapper to make the CPU function look like the GPU one
    def gpu_distance_transform_edt(arr):
        return cpu_distance_transform_edt(arr)


# -----------------------------------------------------------------------------
# CONFIGURATION
# -----------------------------------------------------------------------------
# All settings are in this dictionary. Update the paths to match your system.
# *** IMPORTANT: All 3 paths MUST be FULL, ABSOLUTE paths ***
# -----------------------------------------------------------------------------
CONFIG = {
    'experiment': {
        'name': "noise_10_percent_sparse1_batch32" # <-- EDITED
    },
    'data': {
        # --- 1. Path to the CLEAN sensor data ---
        'sensor_data_csv_path': r'C:\Users\Rohit\Desktop\Intelligent Robotics\G23-Intelligent-Robotics\data\sensor_data_clean.csv',
        
        # --- 2. Paths to your map files ---
        'map_pgm_path': r'C:\Users\Rohit\Desktop\Intelligent Robotics\G23-Intelligent-Robotics\maps\epuck_world_map.pgm',
        'map_yaml_path': r'C:\Users\Rohit\Desktop\Intelligent Robotics\G23-Intelligent-Robotics\maps\epuck_world_map.yaml',
        
        # --- 3. Path to save the output CSV ---
        'output_csv_path': os.path.join(os.path.expanduser('~'), 'Desktop', 'localization_results_10_percent_sparse1_batch32.csv') # <-- EDITED
    },
    'sensors': {
        'lidar': {
            'num_rays': 360,     # Total rays in the scan
            'fov_deg': 360,      # Field of view in degrees
            'max_range': 3.5,    # Max sensor range
            
            # --- THIS IS WHERE THE NOISE IS DEFINED ---
            'noise_std': 0.02,     # 2cm Gaussian noise
            'outlier_rate': 0.40,  # 10% of readings are random
            # --- END OF NOISE DEFINITION ---
            
            'sparsity': 10,       # <-- EDITED: Use 1-in-1 rays (all 360)
        }
    },
    'localization': {
        'likelihood_sigma': 0.1,     # Sharpened sensor model (10cm blur)
        'likelihood_max_dist': 2.0,  # Max distance for likelihood (meters)
        'motion_diffusion_factor': 0.01, # Trust our (perfect) motion model more
        
        'use_pose_hint': True,
        'initial_pose_hint': [-0.46, -0.55],     # (x_m, y_m)
        'initial_uncertainty_m': 0.5,        # Standard deviation (0.5m)
        
        'batch_interval': 32 # Process every 32 steps
    },
    'random_seed': 42
}
# -----------------------------------------------------------------------------
# END OF CONFIGURATION
# -----------------------------------------------------------------------------


# -----------------------------------------------------------------------------
# HELPER FUNCTIONS
# -----------------------------------------------------------------------------

@dataclass
class MapInfo:
    """ Container for map metadata """
    occupancy_grid: cp.ndarray 
    resolution: float
    origin_x: float
    origin_y: float
    width: int
    height: int
    occupied_thresh: float
    free_thresh: float

def load_map(pgm_path, yaml_path):
    """ load map from pgm and yaml files """
    with open(yaml_path, 'r') as f:
        meta = yaml.safe_load(f)

    res = meta['resolution']
    origin = meta['origin']
    ox, oy = origin[0], origin[1]
    occ_thresh = meta.get('occupied_thresh', 0.65)
    free_thresh = meta.get('free_thresh', 0.25)
    negate = meta.get('negate', 0)

    try:
        img = Image.open(pgm_path)
    except FileNotFoundError:
        print(f"FATAL ERROR: Map image file not found at {pgm_path}")
        return None
    img_arr_cpu = np.array(img)

    norm = img_arr_cpu / 255.0
    occ_grid_cpu = np.where(norm > (1 - free_thresh), 0.0,
                        np.where(norm < occ_thresh, 1.0, 0.5))
    h, w = occ_grid_cpu.shape
    occ_grid_gpu = cp.asarray(occ_grid_cpu)

    return MapInfo(
        occupancy_grid=occ_grid_gpu, 
        resolution=res,
        origin_x=ox,
        origin_y=oy,
        width=w,
        height=h,
        occupied_thresh=occ_thresh,
        free_thresh=free_thresh
    )

def apply_sensor_noise(clean_log, config):
    """
    Applies noise to a clean sensor log based on config parameters.
    """
    print("Applying sensor noise based on config file...")
    lidar_config = config['sensors']['lidar']
    noise_std = lidar_config.get('noise_std', 0.0)
    outlier_rate = lidar_config.get('outlier_rate', 0.0)
    max_range = lidar_config.get('max_range', 3.5)
    
    # Seed the random number generator for reproducible noise
    seed = config.get('random_seed', 42)
    rng = np.random.RandomState(seed)
    
    noisy_log = []
    
    for data in clean_log:
        clean_ranges = data['ranges']
        noisy_ranges = clean_ranges.copy()
        
        # 1. Apply Gaussian noise
        if noise_std > 0:
            noise = rng.normal(0.0, noise_std, len(noisy_ranges))
            noisy_ranges += noise
        
        # 2. Apply outlier noise
        if outlier_rate > 0:
            # Find indices to replace
            num_rays = len(noisy_ranges)
            num_outliers = int(num_rays * outlier_rate)
            outlier_indices = rng.choice(num_rays, num_outliers, replace=False)
            
            # Replace with random values between 0 and max_range
            random_values = rng.uniform(0.0, max_range, num_outliers)
            noisy_ranges[outlier_indices] = random_values

        # Clip to valid sensor range
        noisy_ranges = np.clip(noisy_ranges, 0.0, max_range)

        # Create a new data point with the noisy ranges
        noisy_log.append({
            'timestamp': data['timestamp'],
            'ranges': noisy_ranges,
            'pose': data['pose'] # Ground truth pose is unchanged
        })
        
    print(f"Applied Gaussian noise (std={noise_std}) and outlier noise (rate={outlier_rate})")
    return noisy_log


def load_sensor_data(config):
    """
    Loads clean sensor data AND applies noise from the config.
    """
    csv_file_path = config['data']['sensor_data_csv_path']
    # Use 'num_rays' from the config file, not a hardcoded value
    num_rays = config['sensors']['lidar'].get('num_rays', 360) 
    
    clean_sensor_log = []
    
    print(f"Loading CLEAN sensor data from: {csv_file_path}")
    try:
        with open(csv_file_path, 'r') as f:
            reader = csv.reader(f)
            header = next(reader)
            
            try:
                ts_idx = header.index('timestamp')
                range_start_idx = header.index('range_0')
                gt_x_idx = header.index('gt_x')
                gt_y_idx = header.index('gt_y')
                gt_theta_idx = header.index('gt_theta')
            except ValueError as e:
                print(f"Error: Missing required column in CSV: {e}")
                return None
                
            range_end_idx = range_start_idx + num_rays
            if range_end_idx > len(header):
                print(f"Error: CSV header does not have {num_rays} 'range_...' columns.")
                return None

            for row in reader:
                try:
                    data_point = {
                        'timestamp': float(row[ts_idx]),
                        'ranges': np.array([float(r) for r in row[range_start_idx:range_end_idx]]),
                        'pose': (
                            float(row[gt_x_idx]),
                            float(row[gt_y_idx]),
                            float(row[gt_theta_idx])
                        )
                    }
                    clean_sensor_log.append(data_point)
                except (ValueError, IndexError):
                    pass # Skip malformed rows
                    
    except FileNotFoundError:
        print(f"FATAL ERROR: Sensor data file not found at: {csv_file_path}")
        return None
    except Exception as e:
        print(f"Error loading sensor data: {e}")
        return None

    # --- NEW: Apply noise based on the config ---
    noisy_sensor_log = apply_sensor_noise(clean_sensor_log, config)
    
    return noisy_sensor_log

def world_to_grid(x, y, map_info):
    """ world coords (meters) to grid coords (pixels) - CPU """
    grid_x = int(round((x - map_info.origin_x) / map_info.resolution))
    grid_y = int(round((y - map_info.origin_y) / map_info.resolution))
    return grid_x, grid_y

def grid_to_world(grid_x, grid_y, map_info):
    """ grid coords to world coords - CPU """
    x = (grid_x + 0.5) * map_info.resolution + map_info.origin_x
    y = (grid_y + 0.5) * map_info.resolution + map_info.origin_y
    return x, y

def gaussian_2d(x, y, mu_x, mu_y, sigma):
    """
    Calculates 2D Gaussian probability (vectorized for GPU/CPU).
    """
    sigma_sq = sigma**2
    top = cp.exp(-0.5 * (((x - mu_x)**2 / sigma_sq) + ((y - mu_y)**2 / sigma_sq)))
    bottom = 2 * cp.pi * sigma_sq
    return top / bottom

def compute_likelihood_field(map_info, sigma=0.1, max_dist=2.0):
    """ precompute likelihood field for sensor model - GPU/CPU """
    obs = (map_info.occupancy_grid > 0.5).astype(cp.float32)
    dist = gpu_distance_transform_edt(1 - obs)
    dist_m = dist * map_info.resolution
    sig_cells = sigma / map_info.resolution
    field = cp.exp(-0.5 * (dist / sig_cells) ** 2)
    max_cells = max_dist / map_info.resolution
    field[dist > max_cells] = 0.01
    field = field / field.max()
    return field

def compute_scan_likelihood_field(scan_ranges_gpu, scan_angles_gpu, robot_x, robot_y, robot_theta,
                                  map_info, likelihood_field_gpu, max_range):
    """
    Compute total likelihood of a scan using pre-computed likelihood field.
    """
    log_likelihood = 0.0
    num_valid = 0

    robot_x_gpu = cp.float32(robot_x)
    robot_y_gpu = cp.float32(robot_y)
    robot_theta_gpu = cp.float32(robot_theta)

    global_angles = robot_theta_gpu + scan_angles_gpu
    px_gpu = robot_x_gpu + scan_ranges_gpu * cp.cos(global_angles)
    py_gpu = robot_y_gpu + scan_ranges_gpu * cp.sin(global_angles)

    if GPU_ENABLED:
        px_cpu = px_gpu.get()
        py_cpu = py_gpu.get()
        scan_ranges_cpu = scan_ranges_gpu.get()
        likelihood_field_cpu = likelihood_field_gpu.get()
    else:
        px_cpu = px_gpu 
        py_cpu = py_gpu
        scan_ranges_cpu = scan_ranges_gpu
        likelihood_field_cpu = likelihood_field_gpu

    for r, px, py in zip(scan_ranges_cpu, px_cpu, py_cpu):
        # Use max_range from config
        if r < 0.12 or r >= max_range * 0.99: 
            continue
        gx, gy = world_to_grid(px, py, map_info) 
        if 0 <= gx < map_info.width and 0 <= gy < map_info.height:
            p = likelihood_field_cpu[gy, gx]
            p = max(p, 1e-10)
            log_likelihood += np.log(p) 
            num_valid += 1

    if num_valid > 0:
        return log_likelihood / num_valid
    else:
        return 0.0 

# -----------------------------------------------------------------------------
# END OF HELPER FUNCTIONS
# -----------------------------------------------------------------------------


def prediction_step(belief_grid_gpu, motion, map_info, diffusion_factor):
    """
    Updates the belief grid based on a motion model P(x_t | u_t, x_t-1).
    """
    dx_m, dy_m = motion
    
    dx_cells = int(round(dx_m / map_info.resolution))
    dy_cells = int(round(dy_m / map_info.resolution))
    
    predicted_belief = cp.roll(belief_grid_gpu, (dy_cells, dx_cells), axis=(0, 1))
    
    if dy_cells > 0:
        predicted_belief[:dy_cells, :] = 0
    elif dy_cells < 0:
        predicted_belief[dy_cells:, :] = 0
    if dx_cells > 0:
        predicted_belief[:, :dx_cells] = 0
    elif dx_cells < 0:
        predicted_belief[:, dx_cells:] = 0

    uniform_prob = 1.0 / belief_grid_gpu.size
    final_belief = predicted_belief * (1.0 - diffusion_factor) + \
                   (uniform_prob * diffusion_factor)
    
    final_belief[map_info.occupancy_grid > 0.5] = 0.0

    total_prob = cp.sum(final_belief)
    if total_prob > 0:
        return final_belief / total_prob
    else:
        print("  Warning: Prediction step resulted in zero probability. Re-initializing.")
        final_belief = cp.ones_like(belief_grid_gpu)
        final_belief[map_info.occupancy_grid > 0.5] = 0.0
        return final_belief / cp.sum(final_belief)

def correction_step(belief_grid_gpu, sensor_data, map_info, likelihood_field_gpu, config, verbose=False):
    """
    Updates the belief grid based on the sensor model P(z_t | x_t)
    """
    
    lidar_config = config['sensors']['lidar']
    lidar_ranges_gpu = cp.asarray(sensor_data['ranges'])
    
    angles_gpu = cp.linspace(
        -math.radians(lidar_config['fov_deg']) / 2.0,
        math.radians(lidar_config['fov_deg']) / 2.0,
        lidar_config['num_rays']
    )
    
    # Apply sparsity
    sparsity = lidar_config.get('sparsity', 1)
    sparse_indices = cp.arange(0, len(lidar_ranges_gpu), sparsity)
    sparse_ranges_gpu = lidar_ranges_gpu[sparse_indices]
    sparse_angles_gpu = angles_gpu[sparse_indices]

    gt_theta_rad = sensor_data['pose'][2]

    log_likelihood_grid = cp.zeros_like(belief_grid_gpu)
    
    if verbose:
        print("  Running correction step (using likelihood field)...")
    
    y_coords, x_coords = cp.indices(belief_grid_gpu.shape)
    
    if GPU_ENABLED:
        x_coords_cpu = x_coords.get()
        y_coords_cpu = y_coords.get()
    else:
        x_coords_cpu = x_coords 
        y_coords_cpu = y_coords
        
    wx_cpu = (x_coords_cpu + 0.5) * map_info.resolution + map_info.origin_x
    wy_cpu = (y_coords_cpu + 0.5) * map_info.resolution + map_info.origin_y
    
    wx = cp.asarray(wx_cpu)
    wy = cp.asarray(wy_cpu)
    
    log_likelihood_grid = cp.full_like(belief_grid_gpu, -cp.inf) 
    
    valid_mask = (map_info.occupancy_grid < 0.5) & (belief_grid_gpu > 1e-9)
    
    valid_y = y_coords[valid_mask]
    valid_x = x_coords[valid_mask]
    valid_wx = wx[valid_mask]
    valid_wy = wy[valid_mask]

    log_p_values = []
    
    if GPU_ENABLED:
        valid_wx_cpu = valid_wx.get()
        valid_wy_cpu = valid_wy.get()
    else:
        valid_wx_cpu = valid_wx
        valid_wy_cpu = valid_wy
    
    # Get max_range from config
    max_range = lidar_config.get('max_range', 3.5)
        
    for wx_cell, wy_cell in zip(valid_wx_cpu, valid_wy_cpu):
        log_p = compute_scan_likelihood_field(
            sparse_ranges_gpu, sparse_angles_gpu, 
            wx_cell, wy_cell, gt_theta_rad,
            map_info, likelihood_field_gpu,
            max_range # Pass max_range
        )
        log_p_values.append(log_p)
    
    if len(valid_y) > 0:
        log_likelihood_grid[valid_y, valid_x] = cp.asarray(log_p_values)
    
    if verbose:
        print(f"  Correction step done. (Computed likelihood for {len(valid_y)} cells)")

    if len(valid_y) > 0:
        max_log_like = cp.max(log_likelihood_grid[cp.isfinite(log_likelihood_grid)])
    else:
        max_log_like = 0.0 
    
    if not cp.isfinite(max_log_like):
        if verbose:
            print("  Warning: All cells have -inf log-likelihood. Belief may collapse.")
        max_log_like = 0.0

    likelihood_grid = cp.exp(log_likelihood_grid - max_log_like)
    
    updated_belief = belief_grid_gpu * likelihood_grid
    
    total_prob = cp.sum(updated_belief)
    if total_prob > 0:
        return updated_belief / total_prob
    else:
        if verbose:
            print("  Warning: Belief collapsed. Re-initializing.")
        final_belief = cp.ones_like(belief_grid_gpu)
        final_belief[map_info.occupancy_grid > 0.5] = 0.0
        return final_belief / cp.sum(final_belief)

def main():
    """
    This is the main function that runs a single experiment.
    """
    
    # Use the global CONFIG object
    config = CONFIG 
    print(f"Loading configuration from internal CONFIG dictionary...")
    
    # --- 1. Load Map ---
    print("\nLoading map...")
    try:
        map_info = load_map(
            config['data']['map_pgm_path'], 
            config['data']['map_yaml_path']
        )
        if map_info is None: return
        print(f"Map loaded: {map_info.width}x{map_info.height} cells @ {map_info.resolution:.3f} m/cell")
    except Exception as e:
        print(f"An error occurred loading the map: {e}")
        return

    # --- 2. Pre-compute Likelihood Field ---
    print("Pre-computing likelihood field...")
    start_time = time.time()
    likelihood_field_gpu = compute_likelihood_field(
        map_info, 
        config['localization']['likelihood_sigma'],
        config['localization']['likelihood_max_dist']
    )
    print(f"Likelihood field computed in {time.time() - start_time:.2f}s")
    
    # --- 3. Initialize Belief ---
    print("Initializing belief grid...")
    
    use_hint = config['localization'].get('use_pose_hint', False)
    
    if use_hint:
        hint_x, hint_y = config['localization']['initial_pose_hint']
        hint_std = config['localization']['initial_uncertainty_m']
        print(f"Using initial pose hint: ({hint_x}m, {hint_y}m) with {hint_std}m uncertainty.")

        y_coords, x_coords = cp.indices((map_info.height, map_info.width))
        
        if GPU_ENABLED:
            x_coords_cpu = x_coords.get()
            y_coords_cpu = y_coords.get()
        else:
            x_coords_cpu = x_coords
            y_coords_cpu = y_coords
            
        wx_cpu = (x_coords_cpu + 0.5) * map_info.resolution + map_info.origin_x
        wy_cpu = (y_coords_cpu + 0.5) * map_info.resolution + map_info.origin_y
        
        wx = cp.asarray(wx_cpu)
        wy = cp.asarray(wy_cpu)

        belief_grid_gpu = gaussian_2d(wx, wy, hint_x, hint_y, hint_std)
    else:
        print("Using uniform distribution (no pose hint).")
        belief_grid_gpu = cp.ones((map_info.height, map_info.width))
    
    belief_grid_gpu[map_info.occupancy_grid > 0.5] = 0.0
    
    if cp.sum(belief_grid_gpu) == 0:
        print("Error: Map seems to be entirely obstacles (or hint is in a wall).")
        return
        
    belief_grid_gpu = belief_grid_gpu / cp.sum(belief_grid_gpu)
    
    # --- 4. Load Sensor Data (and apply noise) ---
    print("Loading and applying noise to sensor data...")
    sensor_log = load_sensor_data(config)
    if not sensor_log:
        print("Failed to load sensor data. Exiting.")
        return
    print(f"Loaded {len(sensor_log)} full data points.")
    
    # --- BATCH THE DATA ---
    batch_interval = config['localization'].get('batch_interval', 1)
    if batch_interval > 1:
        sensor_log = sensor_log[::batch_interval]
        print(f"Batched data to {len(sensor_log)} steps (1 every {batch_interval}).")
    # --- END OF BATCH ---

    # --- 5. Run Localization Loop ---
    last_gt_pose = None
    total_steps = len(sensor_log)
    all_position_errors = []
    
    # --- Setup CSV file for writing ---
    output_csv_path = config['data']['output_csv_path']
    print(f"Opening output CSV file at: {output_csv_path}")
    try:
        csv_file = open(output_csv_path, 'w', newline='')
        csv_writer = csv.writer(csv_file)
        # Write header
        csv_writer.writerow([
            "timestamp", "gt_x", "gt_y", 
            "est_x", "est_y", "position_error_m", "max_belief"
        ])
    except IOError as e:
        print(f"FATAL ERROR: Could not open CSV file for writing: {e}")
        return
    # --- END OF CSV SETUP ---

    for i, sensor_data in enumerate(sensor_log):
        # We will just print a simple progress bar
        progress = (i + 1) / total_steps
        end_char = '\n' if (i + 1) == total_steps else ''
        print(f"\rProcessing Step {i+1}/{total_steps} [{'#'*int(progress*20):<20}]", end=end_char)
        
        start_step_time = time.time()
        
        current_gt_pose = sensor_data['pose']
        gt_x_m, gt_y_m, gt_theta_rad = current_gt_pose
        
        gt_grid_x, gt_grid_y = world_to_grid(gt_x_m, gt_y_m, map_info)

        # --- Prediction Step (Motion Model) ---
        if last_gt_pose:
            dx_m = current_gt_pose[0] - last_gt_pose[0]
            dy_m = current_gt_pose[1] - last_gt_pose[1]
            motion = (dx_m, dy_m)
            
            belief_grid_gpu = prediction_step(
                belief_grid_gpu, 
                motion, 
                map_info, 
                config['localization']['motion_diffusion_factor']
            )

        # --- Correction Step (Sensor Model) ---
        belief_grid_gpu = correction_step(
            belief_grid_gpu, 
            sensor_data, 
            map_info, 
            likelihood_field_gpu, 
            config,
            verbose=False # Keep it quiet during batch runs
        )

        # --- Find Best Estimate ---
        if GPU_ENABLED:
            belief_grid_cpu = belief_grid_gpu.get()
        else:
            belief_grid_cpu = belief_grid_gpu 
        
        est_grid_y, est_grid_x = np.unravel_index(
            np.argmax(belief_grid_cpu), 
            belief_grid_cpu.shape
        )
        est_x_m, est_y_m = grid_to_world(est_grid_x, est_grid_y, map_info)
        max_prob = np.max(belief_grid_cpu)
        
        # --- Store Error ---
        current_error_m = math.dist((gt_x_m, gt_y_m), (est_x_m, est_y_m))
        all_position_errors.append(current_error_m)

        # --- Write to CSV ---
        csv_writer.writerow([
            sensor_data['timestamp'],
            gt_x_m, gt_y_m,
            est_x_m, est_y_m,
            current_error_m,
            max_prob
        ])

        last_gt_pose = current_gt_pose
    
    # --- Close CSV file ---
    csv_file.close()
    print(f"\nResults saved to {output_csv_path}")

    # --- Calculate and return average error ---
    if all_position_errors: 
        average_error_m = sum(all_position_errors) / len(all_position_errors)
        print(f"Average Position Error: {average_error_m:.3f} m")
    
    print("\n--- Localization Complete ---")

if __name__ == "__main__":
    """
    This is the main entry point to run a single experiment.
    """
    main()